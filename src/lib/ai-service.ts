import type { AIModelConfig, AIModelSettings } from './types';

export interface AIResponse {
  content: string;
  isSimulated: boolean;
  modelName?: string;
  error?: string;
}

export interface AIServiceState {
  isConnected: boolean;
  activeModel: AIModelConfig | null;
  lastError: string | null;
}

const DEFAULT_SYSTEM_PROMPT = `ä½ æ˜¯ OmniCore é’±åŒ…çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“æ³¨äºåŠ å¯†è´§å¸é’±åŒ…ç®¡ç†ã€DeFi ç­–ç•¥å’Œé£é™©åˆ†æã€‚è¯·ç”¨ä¸“ä¸šä¸”å‹å¥½çš„æ–¹å¼å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·:
- æŸ¥è¯¢å’Œç®¡ç†é’±åŒ…ä½™é¢
- åˆ›å»ºå’Œç­¾ç½²äº¤æ˜“
- åˆ†æäº¤æ˜“é£é™©
- ç®¡ç† DeFi ç­–ç•¥å’Œæ”¶ç›Šä¼˜åŒ–
- é…ç½®å¹³å°è®¾ç½®

è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¿æŒä¸“ä¸šä¸”å‹å¥½ã€‚`;

// Simulated response generator (fallback when no real AI is available)
function generateSimulatedResponse(input: string): string {
  const lowerInput = input.toLowerCase();
  
  if (lowerInput.includes('é’±åŒ…') || lowerInput.includes('ä½™é¢') || lowerInput.includes('wallet') || lowerInput.includes('balance')) {
    return 'æˆ‘å·²ç»æ£€æŸ¥äº†æ‚¨çš„é’±åŒ…çŠ¶æ€ã€‚æ‚¨ç›®å‰æœ‰:\n\nğŸ’° **æ€»èµ„äº§**: $231,690.75\n\nä¸»è¦é’±åŒ…:\n- Treasury Vault: $125,432 (Ethereum)\n- Operating Account: $23,234 (Polygon)\n- DeFi Strategy: $8,024 (Arbitrum)\n\néœ€è¦æˆ‘æ‰§è¡Œä»€ä¹ˆæ“ä½œå—ï¼Ÿ';
  }
  
  if (lowerInput.includes('äº¤æ˜“') || lowerInput.includes('è½¬è´¦') || lowerInput.includes('transaction') || lowerInput.includes('transfer')) {
    return 'æˆ‘å¯ä»¥å¸®æ‚¨åˆ›å»ºæ–°äº¤æ˜“ã€‚è¯·æä¾›ä»¥ä¸‹ä¿¡æ¯:\n\n1. å‘é€æ–¹é’±åŒ…\n2. æ¥æ”¶åœ°å€\n3. é‡‘é¢å’Œä»£å¸\n4. äº¤æ˜“æè¿°\n\næˆ–è€…æ‚¨å¯ä»¥è¯´ "ä»Treasury Vaultè½¬è´¦5000 USDCåˆ°ä¾›åº”å•†"ï¼Œæˆ‘ä¼šè‡ªåŠ¨è§£æã€‚';
  }
  
  if (lowerInput.includes('é£é™©') || lowerInput.includes('åˆ†æ') || lowerInput.includes('risk') || lowerInput.includes('analysis')) {
    return 'ğŸ” **é£é™©åˆ†ææŠ¥å‘Š**\n\nå½“å‰å¾…å¤„ç†äº¤æ˜“é£é™©:\n\nâš ï¸ **é«˜é£é™©** - tx-3 (Operating Account)\n- å¤§é¢è½¬è´¦: 25,000 USDT\n- é¦–æ¬¡æ”¶æ¬¾åœ°å€\n- å»ºè®®: éªŒè¯æ”¶æ¬¾æ–¹èº«ä»½\n\nâœ… **ä½é£é™©** - tx-1 (Treasury Vault)\n- å·²çŸ¥æ”¶æ¬¾æ–¹\n- å¸¸è§„äº¤æ˜“æ¨¡å¼\n\néœ€è¦æˆ‘æä¾›æ›´è¯¦ç»†çš„åˆ†æå—ï¼Ÿ';
  }
  
  if (lowerInput.includes('defi') || lowerInput.includes('ç­–ç•¥') || lowerInput.includes('æ”¶ç›Š')) {
    return 'ğŸ“Š **DeFi ç­–ç•¥å»ºè®®**\n\nåŸºäºæ‚¨çš„é£é™©åå¥½ï¼Œæ¨è:\n\n1. **ç¨³å®šå¸å€Ÿè´·** (Aave V3)\n   - APY: 5.2%\n   - é£é™©: ä½\n\n2. **ETH è´¨æŠ¼** (Lido)\n   - APY: 3.8%\n   - é£é™©: ä½\n\n3. **æµåŠ¨æ€§æŒ–çŸ¿** (Uniswap V3)\n   - APY: 12.5%\n   - é£é™©: ä¸­\n\néœ€è¦æˆ‘å¸®æ‚¨é…ç½®è‡ªåŠ¨æŠ•èµ„ç­–ç•¥å—ï¼Ÿ';
  }
  
  return 'æ„Ÿè°¢æ‚¨çš„æé—®ï¼æˆ‘æ˜¯ OmniCore æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨:\n\nâ€¢ ğŸ“Š æŸ¥è¯¢å’Œç®¡ç†é’±åŒ…\nâ€¢ ğŸ’¸ åˆ›å»ºå’Œç­¾ç½²äº¤æ˜“\nâ€¢ ğŸ” åˆ†æäº¤æ˜“é£é™©\nâ€¢ ğŸ“ˆ ç®¡ç† DeFi ç­–ç•¥\nâ€¢ âš™ï¸ é…ç½®å¹³å°è®¾ç½®\n\nè¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ';
}

// Try to call Ollama API
async function callOllamaAPI(
  endpoint: string,
  modelName: string,
  prompt: string,
  systemPrompt: string
): Promise<string> {
  const response = await fetch(endpoint, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: modelName,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
    }),
  });

  if (!response.ok) {
    throw new Error(`Ollama API error: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  return data.response || data.message?.content || '';
}

// Try to call OpenAI-compatible API
async function callOpenAICompatibleAPI(
  endpoint: string,
  modelName: string,
  prompt: string,
  systemPrompt: string,
  maxTokens: number,
  temperature: number,
  apiKey?: string
): Promise<string> {
  const headers: Record<string, string> = {
    'Content-Type': 'application/json',
  };
  
  if (apiKey) {
    headers['Authorization'] = `Bearer ${apiKey}`;
  }

  const response = await fetch(endpoint, {
    method: 'POST',
    headers,
    body: JSON.stringify({
      model: modelName,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: prompt },
      ],
      max_tokens: maxTokens,
      temperature: temperature,
    }),
  });

  if (!response.ok) {
    throw new Error(`API error: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  return data.choices?.[0]?.message?.content || '';
}

// Main AI service function
export async function sendMessage(
  prompt: string,
  settings: AIModelSettings | null
): Promise<AIResponse> {
  // If no settings or no models configured, use simulated response
  if (!settings || settings.models.length === 0) {
    return {
      content: generateSimulatedResponse(prompt),
      isSimulated: true,
    };
  }

  // Find the active model (default or first enabled)
  const activeModel = settings.models.find(
    (m) => m.id === settings.defaultModelId && m.enabled
  ) || settings.models.find((m) => m.enabled);

  if (!activeModel) {
    return {
      content: generateSimulatedResponse(prompt),
      isSimulated: true,
    };
  }

  const systemPrompt = activeModel.systemPrompt || DEFAULT_SYSTEM_PROMPT;

  try {
    let responseContent: string;

    if (activeModel.provider === 'ollama' || activeModel.provider === 'local') {
      responseContent = await callOllamaAPI(
        activeModel.apiEndpoint,
        activeModel.modelName,
        prompt,
        systemPrompt
      );
    } else {
      // OpenAI-compatible API (openai, anthropic, custom)
      responseContent = await callOpenAICompatibleAPI(
        activeModel.apiEndpoint,
        activeModel.modelName,
        prompt,
        systemPrompt,
        activeModel.maxTokens,
        activeModel.temperature,
        activeModel.apiKey
      );
    }

    return {
      content: responseContent,
      isSimulated: false,
      modelName: activeModel.name,
    };
  } catch (error) {
    console.error('AI API error:', error);
    
    // Fallback to simulated response on error
    return {
      content: generateSimulatedResponse(prompt),
      isSimulated: true,
      error: error instanceof Error ? error.message : 'Unknown error',
    };
  }
}

// Check if an AI model endpoint is available
export async function checkModelAvailability(model: AIModelConfig): Promise<boolean> {
  try {
    if (model.provider === 'ollama' || model.provider === 'local') {
      // For Ollama, check the tags endpoint
      const baseUrl = model.apiEndpoint.replace('/api/generate', '').replace('/api/chat', '');
      const response = await fetch(`${baseUrl}/api/tags`, {
        method: 'GET',
        signal: AbortSignal.timeout(5000),
      });
      return response.ok;
    } else {
      // For other APIs, just check if endpoint responds
      const response = await fetch(model.apiEndpoint, {
        method: 'OPTIONS',
        signal: AbortSignal.timeout(5000),
      });
      return response.ok || response.status === 405; // 405 is OK for OPTIONS not allowed
    }
  } catch {
    return false;
  }
}
