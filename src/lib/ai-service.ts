/**
 * AI Service Module
 * Connects to real AI model APIs (Ollama, OpenAI, Anthropic, custom endpoints)
 * Provides actual AI functionality instead of mock responses
 */

import type { AIModelConfig, AIModelSettings, AIMessage } from './types';

export interface AIServiceResponse {
  success: boolean;
  content: string;
  error?: string;
}

export interface StreamCallback {
  onToken: (token: string) => void;
  onComplete: () => void;
  onError: (error: Error) => void;
}

/**
 * Call Ollama API for chat completions
 */
async function callOllamaAPI(
  model: AIModelConfig,
  messages: Array<{ role: string; content: string }>,
  signal?: AbortSignal
): Promise<AIServiceResponse> {
  try {
    const response = await fetch(model.apiEndpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: model.modelName,
        messages: [
          { role: 'system', content: model.systemPrompt },
          ...messages,
        ],
        stream: false,
        options: {
          temperature: model.temperature,
          num_predict: model.maxTokens,
        },
      }),
      signal,
    });

    if (!response.ok) {
      const errorText = await response.text();
      return {
        success: false,
        content: '',
        error: `Ollama API error: ${response.status} - ${errorText}`,
      };
    }

    const data = await response.json();
    
    // Ollama chat API returns response in message.content
    const content = data.message?.content || data.response || '';
    
    return {
      success: true,
      content,
    };
  } catch (error) {
    if (error instanceof Error && error.name === 'AbortError') {
      return {
        success: false,
        content: '',
        error: 'è¯·æ±‚å·²å–æ¶ˆ',
      };
    }
    return {
      success: false,
      content: '',
      error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯',
    };
  }
}

/**
 * Call OpenAI-compatible API (OpenAI, Azure, custom endpoints)
 */
async function callOpenAICompatibleAPI(
  model: AIModelConfig,
  messages: Array<{ role: string; content: string }>,
  signal?: AbortSignal
): Promise<AIServiceResponse> {
  try {
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
    };

    if (model.apiKey) {
      headers['Authorization'] = `Bearer ${model.apiKey}`;
    }

    const response = await fetch(model.apiEndpoint, {
      method: 'POST',
      headers,
      body: JSON.stringify({
        model: model.modelName,
        messages: [
          { role: 'system', content: model.systemPrompt },
          ...messages,
        ],
        max_tokens: model.maxTokens,
        temperature: model.temperature,
        stream: false,
      }),
      signal,
    });

    if (!response.ok) {
      const errorText = await response.text();
      return {
        success: false,
        content: '',
        error: `API error: ${response.status} - ${errorText}`,
      };
    }

    const data = await response.json();
    const content = data.choices?.[0]?.message?.content || '';

    return {
      success: true,
      content,
    };
  } catch (error) {
    if (error instanceof Error && error.name === 'AbortError') {
      return {
        success: false,
        content: '',
        error: 'è¯·æ±‚å·²å–æ¶ˆ',
      };
    }
    return {
      success: false,
      content: '',
      error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯',
    };
  }
}

/**
 * Call Anthropic API
 */
async function callAnthropicAPI(
  model: AIModelConfig,
  messages: Array<{ role: string; content: string }>,
  signal?: AbortSignal
): Promise<AIServiceResponse> {
  try {
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
      'anthropic-version': '2023-06-01',
    };

    if (model.apiKey) {
      headers['x-api-key'] = model.apiKey;
    }

    const response = await fetch(model.apiEndpoint, {
      method: 'POST',
      headers,
      body: JSON.stringify({
        model: model.modelName,
        max_tokens: model.maxTokens,
        system: model.systemPrompt,
        messages: messages.map(m => ({
          role: m.role === 'assistant' ? 'assistant' : 'user',
          content: m.content,
        })),
      }),
      signal,
    });

    if (!response.ok) {
      const errorText = await response.text();
      return {
        success: false,
        content: '',
        error: `Anthropic API error: ${response.status} - ${errorText}`,
      };
    }

    const data = await response.json();
    const content = data.content?.[0]?.text || '';

    return {
      success: true,
      content,
    };
  } catch (error) {
    if (error instanceof Error && error.name === 'AbortError') {
      return {
        success: false,
        content: '',
        error: 'è¯·æ±‚å·²å–æ¶ˆ',
      };
    }
    return {
      success: false,
      content: '',
      error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯',
    };
  }
}

/**
 * Get the default or first enabled model from settings
 */
export function getActiveModel(settings: AIModelSettings): AIModelConfig | null {
  // First try to get the default model
  if (settings.defaultModelId) {
    const defaultModel = settings.models.find(
      m => m.id === settings.defaultModelId && m.enabled
    );
    if (defaultModel) return defaultModel;
  }

  // Otherwise return the first enabled model
  return settings.models.find(m => m.enabled) || null;
}

/**
 * Check if a model endpoint is reachable
 */
export async function testModelConnection(model: AIModelConfig): Promise<{ success: boolean; message: string }> {
  try {
    // For Ollama, check the /api/tags endpoint
    if (model.provider === 'ollama' || model.provider === 'local') {
      const baseUrl = model.apiEndpoint.replace(/\/api\/(generate|chat)$/, '');
      const response = await fetch(`${baseUrl}/api/tags`, {
        method: 'GET',
        signal: AbortSignal.timeout(5000),
      });
      
      if (response.ok) {
        return { success: true, message: 'è¿æ¥æˆåŠŸ' };
      }
      return { success: false, message: `è¿æ¥å¤±è´¥: ${response.status}` };
    }

    // For other providers, try a minimal request
    const testMessages = [{ role: 'user', content: 'test' }];
    const result = await sendMessage(model, testMessages);
    
    if (result.success) {
      return { success: true, message: 'è¿æ¥æˆåŠŸ' };
    }
    return { success: false, message: result.error || 'è¿æ¥å¤±è´¥' };
  } catch (error) {
    return {
      success: false,
      message: error instanceof Error ? error.message : 'è¿æ¥æµ‹è¯•å¤±è´¥',
    };
  }
}

/**
 * Send a message to the AI model and get a response
 */
export async function sendMessage(
  model: AIModelConfig,
  messages: Array<{ role: string; content: string }>,
  signal?: AbortSignal
): Promise<AIServiceResponse> {
  switch (model.provider) {
    case 'ollama':
    case 'local':
      return callOllamaAPI(model, messages, signal);
    case 'openai':
    case 'custom':
      return callOpenAICompatibleAPI(model, messages, signal);
    case 'anthropic':
      return callAnthropicAPI(model, messages, signal);
    default:
      return {
        success: false,
        content: '',
        error: `ä¸æ”¯æŒçš„æä¾›å•†: ${model.provider}`,
      };
  }
}

/**
 * Convert AIMessage array to a format suitable for API calls
 */
export function convertMessagesToAPIFormat(messages: AIMessage[]): Array<{ role: string; content: string }> {
  return messages
    .filter(m => m.role !== 'system')
    .map(m => ({
      role: m.role,
      content: m.content,
    }));
}

/**
 * Fallback response when no AI model is available
 */
export function getFallbackResponse(input: string): string {
  const lowerInput = input.toLowerCase();
  
  if (lowerInput.includes('é’±åŒ…') || lowerInput.includes('ä½™é¢') || lowerInput.includes('wallet') || lowerInput.includes('balance')) {
    return 'âš ï¸ **AIæ¨¡å‹æœªé…ç½®**\n\nè¯·åœ¨"æ¨¡å‹"æ ‡ç­¾é¡µä¸­é…ç½®æ‚¨çš„AIæ¨¡å‹ï¼ˆå¦‚Ollamaã€OpenAIç­‰ï¼‰åï¼Œæˆ‘å°±èƒ½ä¸ºæ‚¨æä¾›æ™ºèƒ½é’±åŒ…ç®¡ç†æœåŠ¡ã€‚\n\nğŸ’¡ æ¨èé…ç½®ï¼š\n- æœ¬åœ°ï¼šOllama + llama3 æˆ– qwen2\n- äº‘ç«¯ï¼šOpenAI GPT-4 æˆ– Claude';
  }
  
  if (lowerInput.includes('äº¤æ˜“') || lowerInput.includes('è½¬è´¦') || lowerInput.includes('transaction') || lowerInput.includes('transfer')) {
    return 'âš ï¸ **AIæ¨¡å‹æœªé…ç½®**\n\nè¯·å…ˆåœ¨"æ¨¡å‹"æ ‡ç­¾é¡µä¸­é…ç½®æ‚¨çš„AIæ¨¡å‹ï¼Œé…ç½®å®Œæˆåæˆ‘å°±èƒ½å¸®æ‚¨åˆ›å»ºå’Œç®¡ç†äº¤æ˜“ã€‚';
  }
  
  if (lowerInput.includes('é£é™©') || lowerInput.includes('åˆ†æ') || lowerInput.includes('risk') || lowerInput.includes('analysis')) {
    return 'âš ï¸ **AIæ¨¡å‹æœªé…ç½®**\n\né£é™©åˆ†æåŠŸèƒ½éœ€è¦AIæ¨¡å‹æ”¯æŒã€‚è¯·åœ¨"æ¨¡å‹"æ ‡ç­¾é¡µä¸­é…ç½®æ‚¨çš„AIæ¨¡å‹ã€‚';
  }
  
  if (lowerInput.includes('defi') || lowerInput.includes('ç­–ç•¥') || lowerInput.includes('æ”¶ç›Š')) {
    return 'âš ï¸ **AIæ¨¡å‹æœªé…ç½®**\n\nDeFiç­–ç•¥æ¨èéœ€è¦AIæ¨¡å‹æ”¯æŒã€‚è¯·åœ¨"æ¨¡å‹"æ ‡ç­¾é¡µä¸­é…ç½®æ‚¨çš„AIæ¨¡å‹ã€‚';
  }
  
  return 'âš ï¸ **AIæ¨¡å‹æœªé…ç½®**\n\næ‚¨å¥½ï¼æˆ‘æ˜¯ OmniCore æ™ºèƒ½åŠ©æ‰‹ã€‚\n\nç›®å‰å°šæœªé…ç½®AIæ¨¡å‹ï¼Œè¯·å‰å¾€"æ¨¡å‹"æ ‡ç­¾é¡µé…ç½®æ‚¨çš„AIæ¨¡å‹ã€‚\n\næ”¯æŒçš„æ¨¡å‹ç±»å‹ï¼š\nâ€¢ ğŸ–¥ï¸ Ollamaï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰\nâ€¢ ğŸŒ OpenAI API\nâ€¢ âš¡ Anthropic Claude\nâ€¢ ğŸ”§ è‡ªå®šä¹‰APIç«¯ç‚¹\n\né…ç½®å®Œæˆåï¼Œæˆ‘å°±èƒ½ä¸ºæ‚¨æä¾›æ™ºèƒ½æœåŠ¡ï¼';
}
